{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd24713",
   "metadata": {},
   "source": [
    "\n",
    "# Breast Cancer Data Analysis (PySpark)\n",
    "\n",
    "**Dataset file:** `/mnt/data/data.csv`\n",
    "\n",
    "\n",
    "**Dataset description:**\n",
    "\n",
    "\n",
    "This notebook performs exploratory data analysis (EDA) on a breast cancer dataset using **PySpark 4.0.1** (compatible code)\n",
    "and common Python libraries (`pandas`, `matplotlib`, `seaborn`). The analysis pipeline mirrors the structure from the\n",
    "`govt_data_analysis_pyspark` notebook you provided, adapted for this medical dataset.\n",
    "\n",
    "**What this notebook includes:**\n",
    "- Spark session initialization (PySpark)\n",
    "- Data loading and preview\n",
    "- Column cleaning (normalize names)\n",
    "- Null / duplicate handling\n",
    "- Automatic identification and casting of numeric columns\n",
    "- Summary statistics and correlations\n",
    "- Visualizations (feature distributions, diagnosis-wise boxplots, correlation heatmap)\n",
    "\n",
    "> Note: The notebook assumes the file `/mnt/data/data.csv` exists (you already uploaded it). If your environment uses a\n",
    "different path, update the `csv_path` variable in the Data Loading cell.\n",
    "\n",
    "**Goal:** Provide clear EDA (summary statistics and visual insights) to help understand patterns in features and how they\n",
    "relate to the target diagnosis (e.g., malignant vs benign).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and Spark session initialization\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create Spark session (compatible with PySpark 4.0.1)\n",
    "spark = SparkSession.builder \\\n",
    "\n",
    "    .appName(\"BreastCancerEDA\") \\\n",
    "\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the CSV file (update if needed)\n",
    "csv_path = \"/mnt/data/data.csv\"\n",
    "\n",
    "# Read CSV without forcing schema (so we can mimic the govt notebook flow)\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=False)\n",
    "\n",
    "print(\"Number of rows:\", df.count())\n",
    "print(\"Number of columns:\", len(df.columns))\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Show sample rows\n",
    "df.show(5, truncate=False)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b10ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize column names: replace spaces, slashes, hyphens, parentheses with underscores\n",
    "def normalize_col_name(c):\n",
    "    return (c.strip()\n",
    "            .replace(' ', '_')\n",
    "            .replace('-', '_')\n",
    "            .replace('/', '_')\n",
    "            .replace('(', '')\n",
    "            .replace(')', '')\n",
    "            .replace('%', 'pct'))\n",
    "\n",
    "old_cols = df.columns\n",
    "new_cols = [normalize_col_name(c) for c in old_cols]\n",
    "\n",
    "for old, new in zip(old_cols, new_cols):\n",
    "    if old != new:\n",
    "        df = df.withColumnRenamed(old, new)\n",
    "\n",
    "print(\"Renamed columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count nulls per column\n",
    "null_counts = df.select([F.count(F.when(F.col(c).isNull() | (F.col(c) == ''), c)).alias(c) for c in df.columns])\n",
    "print(\"Null / empty counts:\")\n",
    "null_counts.show(truncate=False)\n",
    "\n",
    "# Drop exact duplicate rows (if any)\n",
    "before = df.count()\n",
    "df = df.dropDuplicates()\n",
    "after = df.count()\n",
    "print(f\"Dropped {before-after} duplicate rows (if any).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80732d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Heuristic: detect numeric columns by trying to cast to double and checking non-null proportion.\n",
    "numeric_candidates = []\n",
    "for c in df.columns:\n",
    "    # Try casting to double and compute how many values become NULL (i.e., non-numeric)\n",
    "    cast_col = F.col(c).cast('double')\n",
    "    non_null_count = df.select(F.count(F.when(cast_col.isNotNull(), c)).alias('non_null')).collect()[0]['non_null']\n",
    "    total_count = df.count()\n",
    "    # If at least 80% values cast to numeric, consider numeric (threshold can be adjusted)\n",
    "    if total_count > 0 and non_null_count / total_count >= 0.8:\n",
    "        numeric_candidates.append(c)\n",
    "\n",
    "print(\"Numeric-like candidate columns:\", numeric_candidates)\n",
    "\n",
    "# Cast the detected numeric columns to double\n",
    "for c in numeric_candidates:\n",
    "    df = df.withColumn(c, F.col(c).cast(DoubleType()))\n",
    "\n",
    "# Show schema after casting\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d61b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary statistics for numeric columns\n",
    "numeric_cols = [c for c in df.columns if dict(df.dtypes)[c] in ('double', 'int', 'bigint', 'float')]\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "if numeric_cols:\n",
    "    df.select(numeric_cols).describe().toPandas().set_index('summary').T\n",
    "\n",
    "    # Compute medians using approxQuantile\n",
    "    medians = {}\n",
    "    for c in numeric_cols:\n",
    "        med = df.approxQuantile(c, [0.5], 0.01)  # 1% relative error\n",
    "        medians[c] = med[0] if med else None\n",
    "    print(\"Medians (approx):\")\n",
    "    for k, v in medians.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(\"No numeric columns detected to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation heatmap: convert numeric columns to pandas for ease of plotting\n",
    "if numeric_cols:\n",
    "    pdf = df.select(numeric_cols).toPandas()\n",
    "    corr = pdf.corr()\n",
    "    display(corr.head())\n",
    "\n",
    "    # Plot correlation heatmap (matplotlib + seaborn)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', square=True, linewidths=.5)\n",
    "    plt.title('Correlation matrix (numeric features)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No numeric columns available for correlation heatmap.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot distributions for a selection of numeric columns (up to first 6)\n",
    "if numeric_cols:\n",
    "    sample_cols = numeric_cols[:6]\n",
    "    pdf = df.select(sample_cols).toPandas()\n",
    "    pdf.hist(bins=20, figsize=(12, 8))\n",
    "    plt.suptitle('Feature distributions (sample)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No numeric columns to plot distributions.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Look for common target column names (diagnosis, target, class, label)\n",
    "possible_targets = [c for c in df.columns if c.lower() in ('diagnosis', 'target', 'label', 'class')]\n",
    "target_col = possible_targets[0] if possible_targets else None\n",
    "print('Detected target column:', target_col)\n",
    "\n",
    "if target_col and numeric_cols:\n",
    "    # Show counts per class\n",
    "    df.groupBy(target_col).count().show()\n",
    "\n",
    "    # For each numeric column, produce boxplots grouped by target\n",
    "    pdf = df.select([target_col] + numeric_cols).toPandas()\n",
    "    melted = pdf.melt(id_vars=target_col, value_vars=numeric_cols)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='variable', y='value', hue=target_col, data=melted)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Feature distributions by target (boxplots)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No target column detected or no numeric columns available for group comparisons.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afea9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute correlation of numeric features with target if possible (map categorical target to numeric)\n",
    "if target_col and numeric_cols:\n",
    "    # If target is non-numeric, create a mapping\n",
    "    if dict(df.dtypes)[target_col] not in ('double', 'int', 'bigint', 'float'):\n",
    "        distinct_vals = [r[0] for r in df.select(target_col).distinct().collect()]\n",
    "        mapping = {v: i for i, v in enumerate(sorted(distinct_vals))}\n",
    "        print('Mapping target values to integers:', mapping)\n",
    "        mapping_expr = F.create_map([F.lit(x) for kv in sum([[k, v] for k, v in mapping.items()], [])])\n",
    "        df_num_target = df.withColumn('_target_num', mapping_expr[F.col(target_col)])\n",
    "        target_name = '_target_num'\n",
    "    else:\n",
    "        df_num_target = df\n",
    "        target_name = target_col\n",
    "\n",
    "    # compute Pearson correlation between each numeric feature and target\n",
    "    corrs = {}\n",
    "    for c in numeric_cols:\n",
    "        try:\n",
    "            corr_val = df_num_target.stat.corr(c, target_name)\n",
    "            corrs[c] = corr_val\n",
    "        except Exception as e:\n",
    "            corrs[c] = None\n",
    "    # Show sorted correlations by absolute value\n",
    "    sorted_corrs = sorted(corrs.items(), key=lambda x: abs(x[1]) if x[1] is not None else -1, reverse=True)\n",
    "    print('Feature correlations with target (descending by absolute value):')\n",
    "    for k, v in sorted_corrs:\n",
    "        print(f'  {k}: {v}')\n",
    "else:\n",
    "    print('Cannot compute feature-target correlations (missing target or numeric columns).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ec53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optionally, save the cleaned & cast dataframe to Parquet for faster reuse\n",
    "out_path = '/mnt/data/breast_cancer_cleaned.parquet'\n",
    "df.write.mode('overwrite').parquet(out_path)\n",
    "print('Saved cleaned dataframe to', out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc06f45",
   "metadata": {},
   "source": [
    "\n",
    "# Summary & Key Insights\n",
    "\n",
    "\n",
    "*This section summarises the main findings from the exploratory data analysis above.*\n",
    "\n",
    "\n",
    "- **Dataset file used:** `/mnt/data/data.csv`.\n",
    "- **Columns inspected:** see the \"Columns\" output in the Data Loading cell.\n",
    "- **Numeric features:** Detected and cast automatically; summary statistics and medians were computed.\n",
    "- **Correlations:** The correlation heatmap above highlights pairwise relationships between numeric features.\n",
    "- **Target comparisons:** If a `diagnosis` (or `target`/`class`) column was detected, boxplots and target correlations were shown to compare feature distributions between classes (e.g., malignant vs benign).\n",
    "\n",
    "**Suggested next steps (optional):**\n",
    "1. Run feature selection (e.g., remove highly correlated features).\n",
    "2. Train classification models using PySpark MLlib (Logistic Regression, RandomForest) and evaluate with cross-validation.\n",
    "3. Create a summarized PPT and a written report describing the EDA results and any modelling outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "_This notebook was generated to match the structure and operations of your provided `govt_data_analysis_pyspark` notebook, adapted for a breast cancer dataset._\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
